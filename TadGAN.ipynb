{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "TadGAN.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:tf22_kr24]",
   "language": "python",
   "name": "conda-env-tf22_kr24-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jVhHscIMEtMz"
   },
   "source": [
    "### TadGAN for Tensorflow 2.0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NvvVAq07YKLP"
   },
   "source": [
    "#### Part 1\n",
    "- Connect and authenticate user google drive \n",
    "- Data load and prepare"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "E_TAcBTAVPOj",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b3e9e90b-7395-42db-a1ce-89e929794f7d"
   },
   "source": [
    "# drive mount \n",
    "from google.colab import drive\n",
    "import os\n",
    "drive.mount('/content/drive')#, force_remount=True)  # Force_remount 는 강제적으로 해당 경로로 mount 하겠다는 것입니다. "
   ],
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "O8C3rTcZVgzM"
   },
   "source": [
    "os.chdir('/content/drive/My Drive/CoLab/TimeSeries/TadGAN') # 다음 python 실행 부터는 해당 코드만 실행하면 됩니다."
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "R71s6TR4U72u"
   },
   "source": [
    "# load generals\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from utils import plot, plot_ts, plot_rws, plot_error, unroll_ts"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XKNr2O4aYT5m"
   },
   "source": [
    "df = pd.read_csv('nyc_taxi.csv')"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "EBg4-_DoYgwG",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "outputId": "ee3d0355-ebff-4040-e67e-ebcb542a8937"
   },
   "source": [
    "df.head(5)"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1404165600</td>\n",
       "      <td>10844.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1404167400</td>\n",
       "      <td>8127.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1404169200</td>\n",
       "      <td>6210.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1404171000</td>\n",
       "      <td>4656.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1404172800</td>\n",
       "      <td>3820.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    timestamp    value\n",
       "0  1404165600  10844.0\n",
       "1  1404167400   8127.0\n",
       "2  1404169200   6210.0\n",
       "3  1404171000   4656.0\n",
       "4  1404172800   3820.0"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "npWtZAD5X-Hi"
   },
   "source": [
    "def time_segments_aggregate(X, interval, time_column, method=['mean']):\n",
    "    \"\"\"Aggregate values over given time span.\n",
    "    Args:\n",
    "        X (ndarray or pandas.DataFrame):\n",
    "            N-dimensional sequence of values.\n",
    "        interval (int):\n",
    "            Integer denoting time span to compute aggregation of.\n",
    "        time_column (int):\n",
    "            Column of X that contains time values.\n",
    "        method (str or list):\n",
    "            Optional. String describing aggregation method or list of strings describing multiple\n",
    "            aggregation methods. If not given, `mean` is used.\n",
    "    Returns:\n",
    "        ndarray, ndarray:\n",
    "            * Sequence of aggregated values, one column for each aggregation method.\n",
    "            * Sequence of index values (first index of each aggregated segment).\n",
    "    \"\"\"\n",
    "    if isinstance(X, np.ndarray):\n",
    "        X = pd.DataFrame(X)\n",
    "\n",
    "    X = X.sort_values(time_column).set_index(time_column)\n",
    "\n",
    "    if isinstance(method, str):\n",
    "        method = [method]\n",
    "\n",
    "    start_ts = X.index.values[0]\n",
    "    max_ts = X.index.values[-1]\n",
    "\n",
    "    values = list()\n",
    "    index = list()\n",
    "    while start_ts <= max_ts:\n",
    "        end_ts = start_ts + interval\n",
    "        subset = X.loc[start_ts:end_ts - 1]\n",
    "        aggregated = [\n",
    "            getattr(subset, agg)(skipna=True).values\n",
    "            for agg in method\n",
    "        ]\n",
    "        values.append(np.concatenate(aggregated))\n",
    "        index.append(start_ts)\n",
    "        start_ts = end_ts\n",
    "\n",
    "    return np.asarray(values), np.asarray(index)"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bfQE_IlmZv4P"
   },
   "source": [
    "# TimeSegments \n",
    "X, index = time_segments_aggregate(df, interval=1800, time_column='timestamp')"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "49rVid5gYqAI"
   },
   "source": [
    "imp = SimpleImputer()\n",
    "X = imp.fit_transform(X)"
   ],
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "q15IDpMUYyel"
   },
   "source": [
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "X = scaler.fit_transform(X)"
   ],
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YuN0SmzCZ4gx"
   },
   "source": [
    "def rolling_window_sequences(X, index, window_size, target_size, step_size, target_column,\n",
    "                             drop=None, drop_windows=False):\n",
    "    \"\"\"Create rolling window sequences out of time series data.\n",
    "    The function creates an array of input sequences and an array of target sequences by rolling\n",
    "    over the input sequence with a specified window.\n",
    "    Optionally, certain values can be dropped from the sequences.\n",
    "    Args:\n",
    "        X (ndarray):\n",
    "            N-dimensional sequence to iterate over.\n",
    "        index (ndarray):\n",
    "            Array containing the index values of X.\n",
    "        window_size (int):\n",
    "            Length of the input sequences.\n",
    "        target_size (int):\n",
    "            Length of the target sequences.\n",
    "        step_size (int):\n",
    "            Indicating the number of steps to move the window forward each round.\n",
    "        target_column (int):\n",
    "            Indicating which column of X is the target.\n",
    "        drop (ndarray or None or str or float or bool):\n",
    "            Optional. Array of boolean values indicating which values of X are invalid, or value\n",
    "            indicating which value should be dropped. If not given, `None` is used.\n",
    "        drop_windows (bool):\n",
    "            Optional. Indicates whether the dropping functionality should be enabled. If not\n",
    "            given, `False` is used.\n",
    "    Returns:\n",
    "        ndarray, ndarray, ndarray, ndarray:\n",
    "            * input sequences.\n",
    "            * target sequences.\n",
    "            * first index value of each input sequence.\n",
    "            * first index value of each target sequence.\n",
    "    \"\"\"\n",
    "    out_X = list()\n",
    "    out_y = list()\n",
    "    X_index = list()\n",
    "    y_index = list()\n",
    "    target = X[:, target_column]\n",
    "\n",
    "    if drop_windows:\n",
    "        if hasattr(drop, '__len__') and (not isinstance(drop, str)):\n",
    "            if len(drop) != len(X):\n",
    "                raise Exception('Arrays `drop` and `X` must be of the same length.')\n",
    "        else:\n",
    "            if isinstance(drop, float) and np.isnan(drop):\n",
    "                drop = np.isnan(X)\n",
    "            else:\n",
    "                drop = X == drop\n",
    "\n",
    "    start = 0\n",
    "    max_start = len(X) - window_size - target_size + 1\n",
    "    while start < max_start:\n",
    "        end = start + window_size\n",
    "\n",
    "        if drop_windows:\n",
    "            drop_window = drop[start:end + target_size]\n",
    "            to_drop = np.where(drop_window)[0]\n",
    "            if to_drop.size:\n",
    "                start += to_drop[-1] + 1\n",
    "                continue\n",
    "\n",
    "        out_X.append(X[start:end])\n",
    "        out_y.append(target[end:end + target_size])\n",
    "        X_index.append(index[start])\n",
    "        y_index.append(index[end])\n",
    "        start = start + step_size\n",
    "\n",
    "    return np.asarray(out_X), np.asarray(out_y), np.asarray(X_index), np.asarray(y_index)"
   ],
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "V3oekRlSZ6Wn"
   },
   "source": [
    "X, y, X_index, y_index = rolling_window_sequences(X, index, \n",
    "                                                  window_size=100, \n",
    "                                                  target_size=1, \n",
    "                                                  step_size=1,\n",
    "                                                  target_column=0)"
   ],
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PhYaWBwpZ-Az",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "0eb039de-06d2-4b55-97fa-f3ca882c56c7"
   },
   "source": [
    "print(\"Training data input shape: {}\".format(X.shape))\n",
    "print(\"Training data index shape: {}\".format(X_index.shape))\n",
    "print(\"Training y shape: {}\".format(y.shape))\n",
    "print(\"Training y index shape: {}\".format(y_index.shape))\n"
   ],
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data input shape: (10222, 100, 1)\n",
      "Training data index shape: (10222,)\n",
      "Training y shape: (10222, 1)\n",
      "Training y index shape: (10222,)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EjqfKEg9aApL"
   },
   "source": [
    "#### Part 2 \n",
    "\n",
    "- GPU check for TadGAN \n",
    "- Load Tensorflow, Keras, Layers .."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "sTYvrfHTRUbf",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "c16c1127-6760-4c75-c981-54449c95cd60"
   },
   "source": [
    "# Check gpu envrionmental \n",
    "import tensorflow as tf\n",
    "import logging\n",
    "import math\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU') \n",
    "if gpus: \n",
    "    try: \n",
    "        for gpu in gpus: \n",
    "            tf.config.experimental.set_memory_growth(gpu, True) \n",
    "    except RuntimeError as e: \n",
    "        print(e)\n",
    "print (gpus)"
   ],
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "o8ALUCPXEtM-"
   },
   "source": [
    "LOGGER = logging.getLogger(__name__)"
   ],
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7ys30Y68EtM8"
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "#import tensorflow as tf\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import similaritymeasures as sm\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Flatten, Dense, Reshape, UpSampling1D, TimeDistributed\n",
    "from tensorflow.keras.layers import Activation, Conv1D, LeakyReLU, Dropout, Add, Layer\n",
    "from tensorflow.compat.v1.keras.layers import CuDNNLSTM as CUDNNLSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from functools import partial\n",
    "from scipy import integrate, stats"
   ],
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "coQevcffOvWV"
   },
   "source": [
    "# Model Building 개별 함수화"
   ],
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "uj_WD27AOvWV"
   },
   "source": [
    "class RandomWeightedAverage(Layer):\n",
    "    def _merge_function(self, inputs):\n",
    "        alpha = K.random_uniform((64, 1, 1))\n",
    "        return (alpha * inputs[0]) + ((1 - alpha) * inputs[1])"
   ],
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fDOTybi0OvWV"
   },
   "source": [
    "def build_encoder_layer(input_shape, encoder_reshape_shape):    \n",
    "    \n",
    "    input_layer = layers.Input(shape=input_shape)\n",
    "    \n",
    "    x = layers.Bidirectional(CUDNNLSTM(units=100, return_sequences=True))(input_layer)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(20)(x)\n",
    "    x = layers.Reshape(target_shape=encoder_reshape_shape)(x)\n",
    "    model = keras.models.Model(input_layer, x, name='encoder')\n",
    "    \n",
    "    return model\n",
    "\n",
    "def build_generator_layer(input_shape, generator_reshape_shape):\n",
    "    \n",
    "    input_layer = layers.Input(shape=input_shape)\n",
    "    \n",
    "    x = layers.Flatten()(input_layer)\n",
    "    x = layers.Dense(generator_reshape_shape[0])(x)\n",
    "    x = layers.Reshape(target_shape=generator_reshape_shape)(x)\n",
    "    x = layers.Bidirectional(CUDNNLSTM(units=64, return_sequences=True), merge_mode='concat')(x)\n",
    "    x = layers.UpSampling1D(size=2)(x)\n",
    "    x = layers.Bidirectional(CUDNNLSTM(units=64, return_sequences=True), merge_mode='concat')(x)\n",
    "    x = layers.TimeDistributed(layers.Dense(1))(x)\n",
    "    x = layers.Activation(activation='tanh')(x)\n",
    "    model = keras.models.Model(input_layer, x, name='generator')\n",
    "    \n",
    "    return model\n",
    "    \n",
    "\n",
    "def build_critic_x_layer(input_shape):\n",
    "    \n",
    "    input_layer = layers.Input(shape=input_shape)\n",
    "    \n",
    "    x = layers.Conv1D(filters=64, kernel_size=5)(input_layer)\n",
    "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "    x = layers.Dropout(rate=0.25)(x)\n",
    "    x = layers.Conv1D(filters=64, kernel_size=5)(x)\n",
    "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "    x = layers.Dropout(rate=0.25)(x)\n",
    "    x = layers.Conv1D(filters=64, kernel_size=5)(x)\n",
    "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "    x = layers.Dropout(rate=0.25)(x)\n",
    "    x = layers.Conv1D(filters=64, kernel_size=5)(x)\n",
    "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "    x = layers.Dropout(rate=0.25)(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(units=1)(x)\n",
    "    model = keras.models.Model(input_layer, x, name='critic_x')\n",
    "    \n",
    "    return model \n",
    "\n",
    "\n",
    "def build_critic_z_layer(input_shape):\n",
    "    \n",
    "    input_layer = layers.Input(shape=input_shape)\n",
    "    \n",
    "    x = layers.Flatten()(input_layer)\n",
    "    x = layers.Dense(units=100)(x)\n",
    "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "    x = layers.Dropout(rate=0.2)(x)    \n",
    "    x = layers.Dense(units=100)(x)\n",
    "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "    x = layers.Dropout(rate=0.2)(x)  \n",
    "    x = layers.Dense(units=1)(x)\n",
    "    model = keras.models.Model(input_layer, x, name='critic_z')\n",
    "    \n",
    "    return model"
   ],
   "execution_count": 22,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OCdHMC3BOvWW"
   },
   "source": [
    "def wasserstein_loss(y_true, y_pred):\n",
    "#    return tf.reduce_mean(y_true * y_pred)\n",
    "    return K.mean(y_true * y_pred)"
   ],
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "window_size = 100"
   ],
   "metadata": {
    "id": "r_kqDLYdx33S"
   },
   "execution_count": 25,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "z8ZcWpI7OvWW"
   },
   "source": [
    "# Layer Parameters\n",
    "encoder_input_shape = (window_size, 1)\n",
    "generator_input_shape = (20, 1)\n",
    "\n",
    "critic_x_input_shape = (window_size, 1)\n",
    "critic_z_input_shape = (20,1)\n",
    "\n",
    "encoder_reshape_shape = (20, 1)\n",
    "generator_reshape_shape = (window_size//2, 1) # window_size//3 <- 3 is Upsampling size\n",
    "\n",
    "learning_rate = 0.0005\n",
    "\n",
    "# Build Model\n",
    "encoder = build_encoder_layer(input_shape=encoder_input_shape,\n",
    "                              encoder_reshape_shape=encoder_reshape_shape)\n",
    "\n",
    "generator = build_generator_layer(input_shape=generator_input_shape,\n",
    "                                  generator_reshape_shape=generator_reshape_shape)\n",
    "\n",
    "critic_x = build_critic_x_layer(input_shape=critic_x_input_shape)\n",
    "critic_z = build_critic_z_layer(input_shape=critic_z_input_shape)\n",
    "\n",
    "encoder_optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "generator_optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "critic_x_optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "critic_z_optimizer = tf.keras.optimizers.Adam(learning_rate)"
   ],
   "execution_count": 26,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "encoder.summary() # Trace를 입력으로 받아 Latent Vector 생성"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BmoAtlpMyEJM",
    "outputId": "359d9755-29c5-41ab-bfc1-24cf308467fd"
   },
   "execution_count": 27,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 100, 1)]          0         \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 100, 200)         82400     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 20000)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 20)                400020    \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 20, 1)             0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 482,420\n",
      "Trainable params: 482,420\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "generator.summary() # Latent Vector 로 부터 유사한 Trace를 생성 "
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m0OR4I1CyHQP",
    "outputId": "027a4b22-a917-45ff-9fdf-c7288b761204"
   },
   "execution_count": 28,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"generator\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 20, 1)]           0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 20)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 50)                1050      \n",
      "                                                                 \n",
      " reshape_1 (Reshape)         (None, 50, 1)             0         \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 50, 128)          34304     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " up_sampling1d (UpSampling1D  (None, 100, 128)         0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirectio  (None, 100, 128)         99328     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, 100, 1)           129       \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      " activation (Activation)     (None, 100, 1)            0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 134,811\n",
      "Trainable params: 134,811\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "critic_x.summary() # Original input X 에 대한 감시 "
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YfyoOr23yKx_",
    "outputId": "d6d840e8-9858-4c08-d38c-a1e15f0ce2f4"
   },
   "execution_count": 29,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"critic_x\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 100, 1)]          0         \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 96, 64)            384       \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 96, 64)            0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 64)            0         \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 92, 64)            20544     \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 92, 64)            0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 92, 64)            0         \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 88, 64)            20544     \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 88, 64)            0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 88, 64)            0         \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 84, 64)            20544     \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 84, 64)            0         \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 84, 64)            0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 5376)              0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 5377      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 67,393\n",
      "Trainable params: 67,393\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "critic_z.summary()  # Generated 되는 것에 대한 감시"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bpydPLSHyNkI",
    "outputId": "9471b900-1122-4224-c205-5356eba31a8b"
   },
   "execution_count": 30,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"critic_z\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 20, 1)]           0         \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 20)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 100)               2100      \n",
      "                                                                 \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, 100)               0         \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " leaky_re_lu_5 (LeakyReLU)   (None, 100)               0         \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,301\n",
      "Trainable params: 12,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "latent_dim = 20\n",
    "shape = (window_size, 1)"
   ],
   "metadata": {
    "id": "g89LOVyhyQGF"
   },
   "execution_count": 31,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2EPMhkZLOvWW"
   },
   "source": [
    "@tf.function\n",
    "def critic_x_train_on_batch(x, z):\n",
    "    # Loss 크게 이상 없음 \n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        valid_x = critic_x(x)\n",
    "        x_ = generator(z)\n",
    "        fake_x = critic_x(x_)\n",
    "        \n",
    "        # Interpolated \n",
    "        alpha = tf.random.uniform([batch_size, 1, 1], 0.0, 1.0)\n",
    "        interpolated = alpha * x + (1 - alpha) * x_ \n",
    "        \n",
    "        with tf.GradientTape() as gp_tape:\n",
    "            gp_tape.watch(interpolated)\n",
    "            pred = critic_x(interpolated)\n",
    "        \n",
    "        grads = gp_tape.gradient(pred, interpolated)\n",
    "        grad_norm = tf.norm(tf.reshape(grads, (batch_size, -1)), axis=1)\n",
    "        gp_loss = 10.0*tf.reduce_mean(tf.square(grad_norm - 1.))\n",
    "#         grads = tf.square(grads)\n",
    "#         ddx = tf.sqrt(tf.reduce_sum(grads, axis=np.arange(1, len(grads.shape))))\n",
    "#        gp_loss = tf.reduce_mean((1.0 - ddx) ** 2)\n",
    "                \n",
    "        loss1 = wasserstein_loss(-tf.ones_like(valid_x), valid_x)\n",
    "        loss2 = wasserstein_loss(tf.ones_like(fake_x), fake_x)\n",
    "        #loss = tf.add_n([loss1, loss2, gp_loss*10.0])        \n",
    "        loss = loss1 + loss2 + gp_loss\n",
    "#        loss = tf.reduce_mean(loss)\n",
    "                        \n",
    "    gradients = tape.gradient(loss, critic_x.trainable_weights)\n",
    "    critic_x_optimizer.apply_gradients(zip(gradients, critic_x.trainable_weights))\n",
    "    return loss"
   ],
   "execution_count": 32,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "@tf.function\n",
    "def critic_z_train_on_batch(x, z):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        z_ = encoder(x)   \n",
    "        valid_z = critic_z(z)             \n",
    "        fake_z = critic_z(z_) # <- critic_z 의 결과가 매우 않음 \n",
    "        \n",
    "        # Interpolated \n",
    "        alpha = tf.random.uniform([batch_size, 1, 1], 0.0, 1.0)\n",
    "        interpolated = alpha * z + (1 - alpha) * z_ \n",
    "                \n",
    "        with tf.GradientTape() as gp_tape:\n",
    "            gp_tape.watch(interpolated)\n",
    "            pred = critic_z(interpolated, training=True)\n",
    "            \n",
    "        grads = gp_tape.gradient(pred, interpolated)\n",
    "        grad_norm = tf.norm(tf.reshape(grads, (batch_size, -1)), axis=1)\n",
    "        gp_loss = 10.0*tf.reduce_mean(tf.square(grad_norm - 1.))\n",
    "\n",
    "#         grads = tf.square(grads)\n",
    "#         ddx = tf.sqrt(tf.reduce_sum(grads, axis=np.arange(1, len(grads.shape))))\n",
    "#         gp_loss = tf.reduce_mean((1.0 - ddx) ** 2)\n",
    "        \n",
    "        loss1 = wasserstein_loss(-tf.ones_like(valid_z), valid_z)\n",
    "        loss2 = wasserstein_loss(tf.ones_like(fake_z), fake_z) # <- 이게 미친듯이 뜀. \n",
    "        loss = loss1 + loss2 + gp_loss\n",
    "#        loss = tf.reduce_mean(loss)\n",
    "        \n",
    "    gradients = tape.gradient(loss, critic_z.trainable_weights)\n",
    "    critic_z_optimizer.apply_gradients(zip(gradients, critic_z.trainable_weights))\n",
    "    return loss"
   ],
   "metadata": {
    "id": "ZTJplgZCyXVc"
   },
   "execution_count": 33,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "@tf.function\n",
    "def enc_gen_train_on_batch(x, z):\n",
    "    with tf.GradientTape() as enc_tape:\n",
    "        \n",
    "        z_gen_ = encoder(x, training=True)\n",
    "        x_gen_ = generator(z, training=False)        \n",
    "        x_gen_rec = generator(z_gen_, training=False)\n",
    "        \n",
    "        fake_gen_x = critic_x(x_gen_, training=False)\n",
    "        fake_gen_z = critic_z(z_gen_, training=False)\n",
    "        \n",
    "        loss1 = wasserstein_loss(fake_gen_x, -tf.ones_like(fake_gen_x))\n",
    "        loss2 = wasserstein_loss(fake_gen_z, -tf.ones_like(fake_gen_z))\n",
    "        loss3 = 10.0*tf.reduce_mean(tf.keras.losses.MSE(x, x_gen_rec))\n",
    "\n",
    "        enc_loss = loss1 + loss2 + loss3\n",
    "        \n",
    "    gradients_encoder = enc_tape.gradient(enc_loss, encoder.trainable_weights)\n",
    "    encoder_optimizer.apply_gradients(zip(gradients_encoder, encoder.trainable_weights))\n",
    "\n",
    "    with tf.GradientTape() as gen_tape:\n",
    "        \n",
    "        z_gen_ = encoder(x, training=False)\n",
    "        x_gen_ = generator(z, training=True)        \n",
    "        x_gen_rec = generator(z_gen_, training=True)\n",
    "        \n",
    "        fake_gen_x = critic_x(x_gen_, training=False)\n",
    "        fake_gen_z = critic_z(z_gen_, training=False)\n",
    "        \n",
    "        loss1 = wasserstein_loss(fake_gen_x, -tf.ones_like(fake_gen_x))\n",
    "        loss2 = wasserstein_loss(fake_gen_z, -tf.ones_like(fake_gen_z))\n",
    "        loss3 = 10.0*tf.reduce_mean(tf.keras.losses.MSE(x, x_gen_rec))\n",
    "\n",
    "        gen_loss = loss1 + loss2 + loss3\n",
    "        \n",
    "    gradients_generator = gen_tape.gradient(gen_loss, generator.trainable_weights)    \n",
    "    generator_optimizer.apply_gradients(zip(gradients_generator, generator.trainable_weights))    \n",
    "    return enc_loss, gen_loss"
   ],
   "metadata": {
    "id": "9YLCQG7VyadM"
   },
   "execution_count": 34,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Fd-KP-prOvWX"
   },
   "source": [
    "# Train parameters\n",
    "batch_size = 64\n",
    "n_critics = 5\n",
    "epochs = 1000"
   ],
   "execution_count": 35,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "X.shape"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BskyRITqyeng",
    "outputId": "2c199183-7473-44f6-9efc-eb3231d4b014"
   },
   "execution_count": 36,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(10222, 100, 1)"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "tf.config.experimental_run_functions_eagerly(True)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "21-Jg2_6yhKg",
    "outputId": "fc364882-0f4d-4150-ed41-cd1ec72277da"
   },
   "execution_count": 37,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From <ipython-input-37-bdb3352f611a>:1: experimental_run_functions_eagerly (from tensorflow.python.eager.def_function) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.run_functions_eagerly` instead of the experimental version.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "43QYfgRCOvWX"
   },
   "source": [
    "# Data Reshape\n",
    "X = X.reshape((-1, shape[0], 1))\n",
    "X_ = np.copy(X)"
   ],
   "execution_count": 38,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Pp7JdciKOvWX",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "6928452a-c36b-4233-ff12-42b9a97f48ad"
   },
   "source": [
    "# fake = np.ones((batch_size, 1), dtype=np.float32)\n",
    "# valid = -np.ones((batch_size, 1), dtype=np.float32)\n",
    "# delta = np.ones((batch_size, 1), dtype=np.float32)\n",
    "\n",
    "epoch_e_loss = []    \n",
    "epoch_g_loss = []\n",
    "epoch_cx_loss = []\n",
    "epoch_cz_loss = []\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    \n",
    "    np.random.shuffle(X_)\n",
    "    \n",
    "    minibatches_size = batch_size * n_critics  # 64*5 = 320 \n",
    "    num_minibatches = int(X_.shape[0] // minibatches_size)  # 12 \n",
    "    \n",
    "    encoder.trainable = False\n",
    "    generator.trainable = False\n",
    "    \n",
    "    for i in range(num_minibatches):\n",
    "        minibatch = X_[i * minibatches_size: (i + 1) * minibatches_size]\n",
    "                \n",
    "        # Number of Critics \n",
    "        for j in range(n_critics):\n",
    "            \n",
    "            x = minibatch[j * batch_size: (j + 1) * batch_size]\n",
    "            z = tf.random.normal(shape=(batch_size, latent_dim, 1), mean=0.0, stddev=1, dtype=tf.dtypes.float32, seed=1748)\n",
    "            \n",
    "            critic_x.trainable = True\n",
    "            critic_z.trainable = False\n",
    "            epoch_cx_loss.append(critic_x_train_on_batch(x, z))\n",
    "            critic_x.trainable = False\n",
    "            critic_z.trainable = True\n",
    "            epoch_cz_loss.append(critic_z_train_on_batch(x, z))\n",
    "        \n",
    "        critic_z.trainable = False\n",
    "        critic_x.trainable = False\n",
    "        encoder.trainable = True\n",
    "        generator.trainable = True\n",
    "        \n",
    "        enc_loss, gen_loss = enc_gen_train_on_batch(x, z)\n",
    "        epoch_e_loss.append(enc_loss)\n",
    "        epoch_g_loss.append(gen_loss)\n",
    "        \n",
    "    cx_loss = np.mean(np.array(epoch_cx_loss), axis=0)\n",
    "    cz_loss = np.mean(np.array(epoch_cz_loss), axis=0)\n",
    "    e_loss = np.mean(np.array(epoch_e_loss), axis=0)\n",
    "    g_loss = np.mean(np.array(epoch_g_loss), axis=0)\n",
    "    \n",
    "    print('Epoch: {}/{}, [Dx loss: {}] [Dz loss: {}] [E loss: {}] [G loss: {}]'.format(epoch, epochs, cx_loss, cz_loss, e_loss, g_loss))    "
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 1/1000, [Dx loss: -1.1027096509933472] [Dz loss: -2.2487735748291016] [E loss: 3.4475982189178467] [G loss: 2.3799190521240234]\n",
      "Epoch: 2/1000, [Dx loss: -1.481952428817749] [Dz loss: -2.197561502456665] [E loss: -0.20481787621974945] [G loss: -0.8878143429756165]\n",
      "Epoch: 3/1000, [Dx loss: -1.3126425743103027] [Dz loss: -2.0627708435058594] [E loss: -1.9622894525527954] [G loss: -2.4979665279388428]\n",
      "Epoch: 4/1000, [Dx loss: -1.1846270561218262] [Dz loss: -1.9885523319244385] [E loss: -1.7433583736419678] [G loss: -2.1906211376190186]\n",
      "Epoch: 5/1000, [Dx loss: -1.0923662185668945] [Dz loss: -1.926195502281189] [E loss: 0.682047963142395] [G loss: 0.29783156514167786]\n",
      "Epoch: 6/1000, [Dx loss: -1.024757981300354] [Dz loss: -1.9267975091934204] [E loss: 3.1094353199005127] [G loss: 2.7690796852111816]\n",
      "Epoch: 7/1000, [Dx loss: -0.9515321254730225] [Dz loss: -1.915182113647461] [E loss: 4.120485305786133] [G loss: 3.8144795894622803]\n",
      "Epoch: 8/1000, [Dx loss: -0.9013766646385193] [Dz loss: -1.9072667360305786] [E loss: 4.0768208503723145] [G loss: 3.797333002090454]\n",
      "Epoch: 9/1000, [Dx loss: -0.8485717177391052] [Dz loss: -1.908145546913147] [E loss: 3.4152708053588867] [G loss: 3.156345844268799]\n",
      "Epoch: 10/1000, [Dx loss: -0.8230058550834656] [Dz loss: -1.9099783897399902] [E loss: 3.173403739929199] [G loss: 2.9290213584899902]\n",
      "Epoch: 11/1000, [Dx loss: -0.7957254648208618] [Dz loss: -1.8765074014663696] [E loss: 3.6761529445648193] [G loss: 3.4371750354766846]\n",
      "Epoch: 12/1000, [Dx loss: -0.7784877419471741] [Dz loss: -1.8004260063171387] [E loss: 4.83209753036499] [G loss: 4.594958782196045]\n",
      "Epoch: 13/1000, [Dx loss: -0.7613109350204468] [Dz loss: -1.7958649396896362] [E loss: 5.130429267883301] [G loss: 4.812277793884277]\n",
      "Epoch: 14/1000, [Dx loss: -0.7387574315071106] [Dz loss: -1.6717561483383179] [E loss: 5.726762771606445] [G loss: 5.412658214569092]\n",
      "Epoch: 15/1000, [Dx loss: -0.7164310216903687] [Dz loss: -1.6603139638900757] [E loss: 5.870980262756348] [G loss: 5.563958644866943]\n",
      "Epoch: 16/1000, [Dx loss: -0.702281653881073] [Dz loss: -1.6412831544876099] [E loss: 6.617682933807373] [G loss: 6.30385160446167]\n",
      "Epoch: 17/1000, [Dx loss: -0.6773262619972229] [Dz loss: -1.6169136762619019] [E loss: 6.426531791687012] [G loss: 6.105013847351074]\n",
      "Epoch: 18/1000, [Dx loss: -0.6545379757881165] [Dz loss: -1.6246153116226196] [E loss: 6.176011085510254] [G loss: 5.83397912979126]\n",
      "Epoch: 19/1000, [Dx loss: -0.6408946514129639] [Dz loss: -1.5546767711639404] [E loss: 5.640391826629639] [G loss: 5.216849327087402]\n",
      "Epoch: 20/1000, [Dx loss: -0.6312556266784668] [Dz loss: -1.510849118232727] [E loss: 5.671889781951904] [G loss: 5.2454304695129395]\n",
      "Epoch: 21/1000, [Dx loss: -0.617188036441803] [Dz loss: -1.4790327548980713] [E loss: 5.448887348175049] [G loss: 5.024529457092285]\n",
      "Epoch: 22/1000, [Dx loss: -0.6093984246253967] [Dz loss: -1.4747964143753052] [E loss: 5.636142253875732] [G loss: 5.200984477996826]\n",
      "Epoch: 23/1000, [Dx loss: -0.5981696248054504] [Dz loss: -1.4750895500183105] [E loss: 5.909448623657227] [G loss: 5.471402168273926]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "F4LOxxEKOvWb"
   },
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}